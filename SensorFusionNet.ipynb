{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLugQ7p/9Og5pfGISFpW1X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/sensor_fusion/blob/main/SensorFusionNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sNXpb4eDZxQm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SensorFusionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Camera stream (CNN)\n",
        "        self.camera_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3),  # Output: (16, 62, 62)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                   # Output: (16, 31, 31)\n",
        "            nn.Flatten()                       # Output: 16*31*31 = 15,376\n",
        "        )\n",
        "\n",
        "        # LiDAR stream (PointNet-like)\n",
        "        self.lidar_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 64),  # Input: XYZ coordinates per point\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128) # Output: 128 features per point\n",
        "        )\n",
        "\n",
        "        # Fusion layer (adjusted input dimension)\n",
        "        self.fusion = nn.Linear(15376 + 128, 256)  # 15,376 (camera) + 128 (LiDAR)\n",
        "        self.output = nn.Linear(256, 4)            # Bounding box prediction\n",
        "\n",
        "    def forward(self, camera_img, lidar_points):\n",
        "        # Process camera\n",
        "        camera_features = self.camera_cnn(camera_img)  # Shape: [batch, 15376]\n",
        "\n",
        "        # Process LiDAR\n",
        "        lidar_features = self.lidar_mlp(lidar_points)  # Shape: [batch, num_points, 128]\n",
        "        lidar_features = torch.max(lidar_features, dim=1)[0]  # Global feature: [batch, 128]\n",
        "\n",
        "        # Concatenate and predict\n",
        "        fused = torch.cat([camera_features, lidar_features], dim=1)  # [batch, 15376+128]\n",
        "        fused = self.fusion(fused)\n",
        "        return self.output(fused)\n",
        "\n",
        "# Test the corrected model\n",
        "model = SensorFusionNet()\n",
        "camera_input = torch.randn(1, 3, 64, 64)  # Batch of 1 RGB image\n",
        "lidar_input = torch.randn(1, 100, 3)       # Batch of 1 LiDAR point cloud (100 points)\n",
        "predicted_bbox = model(camera_input, lidar_input)  # No error!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_bbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0rDqH0ca9tJ",
        "outputId": "e59d328b-45a2-416e-d35d-d8a103c539ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3337, -0.3898, -0.2344,  0.0316]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}